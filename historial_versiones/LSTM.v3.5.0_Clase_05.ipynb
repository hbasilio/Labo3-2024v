{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#34656d;font-family:'Comic Sans MS';text-align:center;border-radius:5px;\">\n",
    "<strong>LSTM - Clase 5 - V5.0</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#34656d;font-family:'Comic Sans MS';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con LSTM solo obteniento el resultado de los top 10 productos para ver si esta bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   periodo  customer_id  product_id  plan_precios_cuidados  cust_request_qty  cust_request_tn       tn\n",
      "0   201701        10234       20524                      0                 2          0.05300  0.05300\n",
      "1   201701        10032       20524                      0                 1          0.13628  0.13628\n",
      "2   201701        10217       20524                      0                 1          0.03028  0.03028\n",
      "3   201701        10125       20524                      0                 1          0.02271  0.02271\n",
      "4   201701        10012       20524                      0                11          1.54452  1.54452\n",
      "   product_id\n",
      "0       20001\n",
      "1       20002\n",
      "2       20003\n",
      "3       20004\n",
      "4       20005\n"
     ]
    }
   ],
   "source": [
    "# Lee el archivo como un DataFrame\n",
    "data_full  = pd.read_csv('data/sell-in.csv', delimiter='\\t')\n",
    "df_pid_validos  = pd.read_csv('data/productos_a_predecir.txt')\n",
    "\n",
    "# Ajustar el ancho máximo de las columnas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Ajustar el ancho máximo de la visualización\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "print(data_full.head())\n",
    "print(df_pid_validos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2945818 entries, 0 to 2945817\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   periodo                int64  \n",
      " 1   customer_id            int64  \n",
      " 2   product_id             int64  \n",
      " 3   plan_precios_cuidados  int64  \n",
      " 4   cust_request_qty       int64  \n",
      " 5   cust_request_tn        float64\n",
      " 6   tn                     float64\n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 157.3 MB\n"
     ]
    }
   ],
   "source": [
    "data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lista product_ids_validos tiene 780 registros.\n"
     ]
    }
   ],
   "source": [
    "# Armado de la lista de productos validos a predecir para el periodo\n",
    "product_ids_validos = df_pid_validos['product_id'].tolist()\n",
    "\n",
    "# Ver cuántos registros tiene la lista\n",
    "num_registros = len(product_ids_validos)\n",
    "print(f\"La lista product_ids_validos tiene {num_registros} registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame filtrado tiene 2293481 registros.\n",
      "Todos los product_id en el DataFrame filtrado son válidos.\n",
      "Número de registros por product_id en el DataFrame filtrado:\n",
      "product_id\n",
      "20111    7973\n",
      "20122    7950\n",
      "20120    7537\n",
      "20326    7397\n",
      "20132    7199\n",
      "         ... \n",
      "21267      67\n",
      "21252      67\n",
      "21276      64\n",
      "20886      63\n",
      "20953      62\n",
      "Name: count, Length: 780, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filtrar el DataFrame original para quedarse solo con los product_id válidos\n",
    "data = data_full[data_full['product_id'].isin(product_ids_validos)]\n",
    "\n",
    "# Ver cuántos registros tiene el DataFrame filtrado\n",
    "num_registros_filtrados = len(data)\n",
    "print(f\"El DataFrame filtrado tiene {num_registros_filtrados} registros.\")\n",
    "\n",
    "# Verificar que todos los product_id en el DataFrame filtrado están en la lista de productos válidos\n",
    "productos_unicos_filtrados = data['product_id'].unique()\n",
    "productos_invalidos = [pid for pid in productos_unicos_filtrados if pid not in product_ids_validos]\n",
    "\n",
    "if len(productos_invalidos) == 0:\n",
    "    print(\"Todos los product_id en el DataFrame filtrado son válidos.\")\n",
    "else:\n",
    "    print(f\"Se encontraron productos no válidos en el DataFrame filtrado: {productos_invalidos}\")\n",
    "\n",
    "# (Opcional) Ver cuántos registros hay por cada product_id\n",
    "registros_por_producto = data['product_id'].value_counts()\n",
    "print(\"Número de registros por product_id en el DataFrame filtrado:\")\n",
    "print(registros_por_producto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2293481 entries, 0 to 2945817\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   periodo                int64  \n",
      " 1   customer_id            int64  \n",
      " 2   product_id             int64  \n",
      " 3   plan_precios_cuidados  int64  \n",
      " 4   cust_request_qty       int64  \n",
      " 5   cust_request_tn        float64\n",
      " 6   tn                     float64\n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 140.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20001, 20002, 20003]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener los 10 product_id con mayor cantidad de ventas totales\n",
    "top_product_ids = data.groupby('product_id')['tn'].sum().sort_values(ascending=False).head(3).index.tolist()\n",
    "top_product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el DataFrame \"data\" para incluir solo los \"n\" product_id más importantes\n",
    "data_top = data[data['product_id'].isin(top_product_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las ventas totales por product_id y ordenarlas de mayor a menor\n",
    "product_sales = data.groupby('product_id')['tn'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 product_id por ventas totales:\n",
      "product_id\n",
      "20001    50340.39558\n",
      "20002    36337.25439\n",
      "20003    32004.15274\n",
      "Name: tn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el listado de los 10 product_id con mayor cantidad de ventas totales\n",
    "print(\"Top 3 product_id por ventas totales:\")\n",
    "print(product_sales.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ventas totales: 1122605.22\n",
      "Ventas totales de los 10 product_id más importantes: 118681.80\n",
      "Participación de mercado de los 10 product_id más importantes: 10.57%\n",
      "Ventas promedio por product_id de los 10 más importantes: 26559.14\n"
     ]
    }
   ],
   "source": [
    "# Calcular métricas de control\n",
    "total_sales = data['tn'].sum()\n",
    "top_10_sales = product_sales.head(3).sum()\n",
    "market_share = (top_10_sales / total_sales) * 100\n",
    "\n",
    "print(f\"\\nVentas totales: {total_sales:.2f}\")\n",
    "print(f\"Ventas totales de los 10 product_id más importantes: {top_10_sales:.2f}\")\n",
    "print(f\"Participación de mercado de los 10 product_id más importantes: {market_share:.2f}%\")\n",
    "print(f\"Ventas promedio por product_id de los 10 más importantes: {product_sales.head(10).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecciona el método de normalización:\n",
      "1. Escalador estándar (StandardScaler)\n",
      "2. Escalador MinMax (MinMaxScaler)\n",
      "3. Sin normalización\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa el número de la opción (1-3):  2\n"
     ]
    }
   ],
   "source": [
    "# Elegir el método de normalización\n",
    "print(\"Selecciona el método de normalización:\")\n",
    "print(\"1. Escalador estándar (StandardScaler)\")\n",
    "print(\"2. Escalador MinMax (MinMaxScaler)\")\n",
    "print(\"3. Sin normalización\")\n",
    "choice = int(input(\"Ingresa el número de la opción (1-3): \"))\n",
    "\n",
    "if choice == 1:\n",
    "    scaler = StandardScaler()\n",
    "elif choice == 2:\n",
    "    scaler = MinMaxScaler()\n",
    "else:\n",
    "    scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir y entrenar el modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hbasilio\\Documents\\PERSONAL\\Maestria\\Materias\\LABO_3\\ambiente-virtual\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2ms/step - loss: 6.0413\n",
      "Epoch 2/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 2ms/step - loss: 0.3785\n",
      "Epoch 3/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 2ms/step - loss: 0.2225\n",
      "Epoch 4/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2ms/step - loss: 0.1500\n",
      "Epoch 5/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 4ms/step - loss: 0.0616\n",
      "Epoch 6/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3ms/step - loss: 0.0338\n",
      "Epoch 7/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 3ms/step - loss: 0.0564\n",
      "Epoch 8/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 3ms/step - loss: 0.0296\n",
      "Epoch 9/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2ms/step - loss: 0.0618\n",
      "Epoch 10/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3ms/step - loss: 0.0294\n",
      "Epoch 11/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2ms/step - loss: 0.0228\n",
      "Epoch 12/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2ms/step - loss: 0.0185\n",
      "Epoch 13/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2ms/step - loss: 0.0332\n",
      "Epoch 14/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 3ms/step - loss: 0.0162\n",
      "Epoch 15/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2ms/step - loss: 0.0189\n",
      "Epoch 16/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2ms/step - loss: 0.0094\n",
      "Epoch 17/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2ms/step - loss: 0.0076\n",
      "Epoch 18/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2ms/step - loss: 0.0221\n",
      "Epoch 19/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2ms/step - loss: 0.0039\n",
      "Epoch 20/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2ms/step - loss: 0.0077\n",
      "Epoch 21/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2ms/step - loss: 0.0132\n",
      "Epoch 22/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 3ms/step - loss: 0.0018\n",
      "Epoch 23/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2ms/step - loss: 0.0340\n",
      "Epoch 24/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 2ms/step - loss: 0.0065\n",
      "Epoch 25/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2ms/step - loss: 0.0071\n",
      "Epoch 26/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2ms/step - loss: 0.0048\n",
      "Epoch 27/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2ms/step - loss: 0.0117\n",
      "Epoch 28/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2ms/step - loss: 0.0073\n",
      "Epoch 29/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2ms/step - loss: 0.0032\n",
      "Epoch 30/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 31/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2ms/step - loss: 0.0047\n",
      "Epoch 32/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2ms/step - loss: 0.0035\n",
      "Epoch 33/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2ms/step - loss: 0.0038\n",
      "Epoch 34/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2ms/step - loss: 0.0024\n",
      "Epoch 35/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2ms/step - loss: 0.0024\n",
      "Epoch 36/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 2ms/step - loss: 7.2299e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m71672/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 2ms/step - loss: 0.0020\n",
      "Epoch 38/100\n",
      "\u001b[1m69667/71672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0047"
     ]
    }
   ],
   "source": [
    "# Inicializar el scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Preparar los datos de entrenamiento\n",
    "X = data['tn'].values.reshape(-1, 1)  # Reshape a 2D para el scaler\n",
    "y = data['tn'].values.reshape(-1, 1)\n",
    "\n",
    "# Escalar los datos\n",
    "if not hasattr(scaler, 'n_features_in_'):\n",
    "    scaler.fit(X)\n",
    "X_normalized = scaler.transform(X)\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_lstm = X_normalized.reshape(-1, 1, 1)\n",
    "\n",
    "# Definir y compilar el modelo LSTM\n",
    "# model = Sequential()\n",
    "#model.add(LSTM(50, return_sequences=False, input_shape=(1, 1)))\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Definir la capa de entrada\n",
    "inputs = Input(shape=(1, 1))  # (timesteps, features)\n",
    "\n",
    "# Definir las capas del modelo\n",
    "x = LSTM(50, return_sequences=False)(inputs)\n",
    "outputs = Dense(1)(x)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Entrenar el modelo\n",
    "model.fit(X_lstm, y, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Crear una lista con los product_id más importantes\n",
    "product_ids = top_product_ids\n",
    "\n",
    "# Crear un DataFrame vacío para las predicciones\n",
    "predictions = pd.DataFrame(columns=['product_id', 'tn'])\n",
    "\n",
    "# Hacer predicciones para cada product_id\n",
    "for product_id in product_ids:\n",
    "    product_data = data[data['product_id'] == product_id]\n",
    "    product_data = product_data.sort_values('periodo')\n",
    "    \n",
    "    # Obtener el último valor de tn\n",
    "    last_tn = product_data['tn'].values[-1]\n",
    "    \n",
    "    # Normalizar el último valor de tn\n",
    "    last_tn_reshaped = np.array([last_tn]).reshape(-1, 1)\n",
    "    last_tn_normalized = scaler.transform(last_tn_reshaped)\n",
    "    last_tn_lstm = last_tn_normalized.reshape(1, 1, 1)\n",
    "    \n",
    "    # Hacer la predicción\n",
    "    prediction_normalized = model.predict(last_tn_lstm)[0][0]\n",
    "    \n",
    "    # Revertir la escala de la predicción\n",
    "    prediction = scaler.inverse_transform(np.array([prediction_normalized]).reshape(-1, 1))[0][0]\n",
    "    \n",
    "    # Agregar la predicción al DataFrame\n",
    "    new_row = pd.DataFrame({'product_id': [product_id], 'tn': [prediction]})\n",
    "    predictions = pd.concat([predictions, new_row], ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame de predicciones en un archivo CSV\n",
    "predictions.to_csv('LSTM_top10.v5.0.2.csv', index=False)\n",
    "print(\"Predicciones guardadas en 'LSTM_top10.v5.0.2.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "X = data['tn'].values.reshape(-1, 1)  # Reshape a 2D para el scaler\n",
    "y = data['tn'].values.reshape(-1, 1)\n",
    "\n",
    "# Escalar los datos\n",
    "if scaler is not None:\n",
    "    if not hasattr(scaler, 'n_features_in_'):\n",
    "        scaler.fit(X)\n",
    "    X_normalized = scaler.transform(X)\n",
    "else:\n",
    "    X_normalized = X\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_lstm = X_normalized.reshape(-1, 1, 1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_lstm, y, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scaler, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Verifica si el scaler ya ha sido ajustado\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m         \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Si no ha sido ajustado, ajústalo primero\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     X_normalized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_normalized, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\PERSONAL\\Maestria\\Materias\\LABO_3\\ambiente-virtual\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\PERSONAL\\Maestria\\Materias\\LABO_3\\ambiente-virtual\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\PERSONAL\\Maestria\\Materias\\LABO_3\\ambiente-virtual\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32m~\\Documents\\PERSONAL\\Maestria\\Materias\\LABO_3\\ambiente-virtual\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\Documents\\PERSONAL\\Maestria\\Materias\\LABO_3\\ambiente-virtual\\Lib\\site-packages\\sklearn\\utils\\validation.py:1053\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1051\u001b[0m     )\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1059\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1060\u001b[0m         array,\n\u001b[0;32m   1061\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1062\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1063\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1064\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "# Crear una lista con los product_id más importantes\n",
    "product_ids = top_product_ids\n",
    "\n",
    "# Crear un DataFrame vacío para las predicciones\n",
    "predictions = pd.DataFrame(columns=['product_id', 'tn'])\n",
    "\n",
    "# Hacer predicciones para cada product_id\n",
    "for product_id in product_ids:\n",
    "    product_data = data[data['product_id'] == product_id]\n",
    "    product_data = product_data.sort_values('periodo')\n",
    "    \n",
    "    # Obtener el último valor de tn\n",
    "    last_tn = product_data['tn'].values[-1]\n",
    "    \n",
    "    # Normalizar el último valor de tn (si se seleccionó una opción de normalización)\n",
    "    if scaler is not None:\n",
    "        last_tn_normalized = scaler.transform(np.array([last_tn]).reshape(1, 1))[0][0]\n",
    "        \n",
    "        # Hacer la predicción para febrero de 2020\n",
    "        prediction_normalized = model.predict(np.array([last_tn_normalized]).reshape(1, 1, 1))[0][0]\n",
    "        \n",
    "        # Revertir la escala de la predicción\n",
    "        prediction = scaler.inverse_transform(np.array([prediction_normalized]).reshape(1, 1))[0][0]\n",
    "    else:\n",
    "        # Hacer la predicción sin normalizar\n",
    "        prediction = model.predict(np.array([last_tn]).reshape(1, 1, 1))[0][0]\n",
    "    \n",
    "    # Agregar la predicción al DataFrame\n",
    "    new_row = pd.DataFrame({'product_id': [product_id], 'tn': [prediction]})\n",
    "    predictions = pd.concat([predictions, new_row], ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame de predicciones en un archivo CSV\n",
    "predictions.to_csv('LSTM_top10.v5.0.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#34656d;font-family:'Comic Sans MS';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________</strong></p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
